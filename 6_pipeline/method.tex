\section{Methodology}\label{sec:chp6:method}

Our \ac{mpmri} \ac{cad} system consists of seven different steps: pre-processing, segmentation, registration, feature detection, balancing, feature selection/extraction, and finally classification.
%% It should be noted that \ac{cad} system designed deals with multiparametric \ac{mri} data. 

\subsection{Pre-processing}\label{subsec:chp6:method:PP}

\begin{figure}
  \hspace*{\fill}
  \subfigure[]{\label{fig:adcpdf1}\includegraphics[width=.3\textwidth]{6_pipeline/figures/adc_pdf.pdf}}
  \hfill
  \subfigure[]{\label{fig:adcpdf2}\includegraphics[width=.3\textwidth]{6_pipeline/figures/adc_pdf_2.pdf}}
  \hfill
  \subfigure[]{\label{fig:adcpdf3}\includegraphics[width=.3\textwidth]{6_pipeline/figures/adc_pdf_3.pdf}}
  \hspace*{\fill}
  \caption[Illustration of the \acs*{pdf} of the \acs*{adc} coefficient within the prostate.]{Illustration of the variability of the \acs*{pdf} of the \acs*{adc} coefficient within the prostate for 3 patients.}
  \label{fig:adcpdf}
\end{figure}


The reader can refer to \acs{sec}\,\ref{subsec:chp3img-reg:prepro} to have an extensive overview of the state-of-the-art methods used to pre-process \ac{mpmri} data.
Three types of pre-processing are used for \ac{mri} images: (i) noise filtering, (ii) bias correction, and (iii) standardization/normalization.
Our dataset is based on \SI{3}{\tesla} images without endorectal coil and therefore, the two first types of correction have not been considered as necessary.
Normalization is, however, a crucial step to reduce the inter-patient variations which allows to improve the learning during the classification stage.
\Ac{chp}~\ref{chap:5} presented two normalization methods to pre-process \ac{t2w}-\ac{mri} and \ac{dce}-\ac{mri}, respectively.
Therefore, we used these methods to standardize these images.
Regarding the \ac{adc} map normalization, the \ac{pdf} within the prostate does not follow a known distribution as depicted in \acs{fig}\,\ref{fig:adcpdf}.
Thus, one cannot use a parametric model to normalize these images and a non-parametric piecewise-linear normalization~\cite{Nyul2000} is the best option for this case.

Additionally, the \ac{mrsi} modality requires a specific pre-processing based on signal processing rather than image processing.
Therefore, the \ac{mrsi} modality has been pre-process to correct the phase, baseline, and frequency.
Regarding the problem of phase correction and frequency alignment, we use the most efficient method of the state-of-the-art review in \acs{sec}\,\ref{subsec:chp3img-reg:prepro}.
Indeed, as \citeauthor{Parfait2012} and \citeauthor{trigui2017automatic}~\cite{Parfait2012,trigui2016classification,trigui2017automatic}, the phase of each \ac{mrsi} spectra is corrected using the approach of \citeauthor{Chen2002}~\cite{Chen2002}.
Along the same line, the frequency shift of each spectra is corrected by aligning to \SI{4.65}{\ppm} the maximum of an inferred function fitted to the residuals of water, using a Voigt profile as in \acs{eq}\,\eqref{eq:voigt}.

\begin{equation}
  V(x; \sigma, \gamma) = \frac{\mathbf{R} \left[ w(z) \right]}{\sigma \sqrt{2\pi}} \ ,
  \label{eq:voigt}
\end{equation}

\noindent where $\mathbf{R} \left[ w(z) \right]$ is the real part of the Faddeva function for $z = \frac{x + i \gamma}{\sigma \sqrt{2}}$.

By assessing the qualitative results obtained in~\cite{Parfait2010}, the baseline correction method used by \citeauthor{Parfait2012} and \citeauthor{trigui2017automatic} does not provide an optimal solution for that matter.
The iterative low-pass filter enforce to much the smoothness of the baseline.
\citeauthor{xi2008baseline} propose a baseline detection derived from a parametric smoothing model~\cite{xi2008baseline}.
The \ac{nmr} signal is formalized as a sum of a pure signal, the baseline function, and an additive Gaussian noise such as:

\begin{equation}
  y_i = b_i + \mu_i e^{n_i} + \varepsilon_i \ ,
  \label{eq:methodBaselineDetectionModel}
\end{equation}

\noindent where $y_i$ is the \ac{nmr} signal, $b_i$ is the baseline, $\mu_i$ is the true signal, and $n_i$ and $\varepsilon_i$ are Gaussian noises.

\citeauthor{xi2008baseline} propose to find the baseline function through an iterative optimization by maximizing the following cost function:

\begin{equation}
  F(b) = \sum_{i = 1}^{N} b_i - \frac{A^{*} N^4}{\sigma} \sum_{i = 1}^{N} (b_{i+1} + b_{i-1} - 2 b_i)^2 - \frac{1.25 B^{*}}{\sigma} \sum_{i = 1}^{N} (b_i - \gamma_i)^2 g(b_i - \gamma_i) \ ,
  \label{eq:methodBaselineDetectionCostFunction}
\end{equation}

\noindent where $g(b_i - \gamma_i)$ is the Heaviside function, $A^*$ and $B^*$ are the terms controlling the smoothness and negative penalties, respectively, $\sigma$ is an estimation of the standard deviation of the noise, and $N$ is the total number of points in the \ac{mrsi} signal.

The standard deviation of the noise $\sigma$ is estimated as in~\cite{xi2008baseline}, and the $A^{*}$ and $B^{*}$ are empirically set to $5 \times 10^{-6}$ and $100$, respectively, for all the \ac{mrsi} signal.
Setting these parameters allows to obtain an estimation of a smooth and possibly negative baseline, required by the aspect of the citrate peak in our \ac{mrsi} acquisition, as depicted in \acs{fig}\,\ref{fig:baselinemrsi}.

\begin{figure}
  \centering
  \includegraphics[width=0.5\linewidth]{6_pipeline/figures/baseline_mrsi.pdf}
  \caption{Illustration of the detection of the baseline on an \acs*{mrsi} spectrum.}
  \label{fig:baselinemrsi}
\end{figure}

Additionally, each \ac{mrsi} spectrum is normalized using the L$_2$ norm, which has been shown to be the most efficient normalization method in \ac{mrsi} as discussed in \acs{sec}\,\ref{subsec:chp3img-reg:prepro}.

\subsection{Segmentation and registration}\label{subsec:chp6:method:Seg-Reg}

For this study, no segmentation method has been developed and the manual segmentation given by our radiologist has been used.
The prostate is suffering, however, from a misalignment between the different \ac{mri} modalities.
Therefore, three registrations have been developed to: (i) the patient motion during the \ac{dce}-\ac{mri} acquisition, (ii) the patient motion between the \ac{t2w}-\ac{mri} and the \ac{dce}-\ac{mri} acquisitions, and (iii) the patient motion between the \ac{t2w}-\ac{mri} and the \ac{adc} map acquisition.
All registrations are implemented in C++ using \ac{itk}.

The \ac{dce}-\ac{mri} acquisition being dynamic, some intra-patient might occur during the acquisition.
For each serie of this dynamic acquisition, each 3D volume is registered to the first volume acquired, to remove the residual motion.
The appearance in the \ac{dce}-\ac{mri} images, however, varies due to the presence or not of the contrast media.
Therefore, the metric chosen to be minimized is the \ac{mi} and the geometric transform has been set to a rigid transform.
The optimization is performed using a regular step gradient descent.

Once the intra-patient motions corrected, a registration to correct the alignment between the \ac{t2w}-\ac{mri} and the \ac{dce}-\ac{mri} acquisitions is performed.
For that matter, the prostate has been segmented in both modalities --- \ac{t2w}-\ac{mri} and \ac{dce}-\ac{mri} --- to create two binary masks.
Therefore, these 3D binary masks are directly registered using the \ac{mse} metric.
Unlike the previous registration, we use a more complex geometric transform by successively finding a rigid transformation, a coarse elastic transformation, and a fine elastic transformation.
B-splines transformation is used as the elastic transform.
These successive transformations allow to get a good initialization for the next transformation.
The transformation is inferred by minimizing the cost function using a regular step gradient descent.

The \ac{t2w}-\ac{mri} and \ac{adc} map acquisitions are identically registered than the the \ac{t2w}-\ac{mri} and the \ac{dce}-\ac{mri} modalities.
Additionally, the \ac{cap}, \ac{pz}, and \ac{cg} are segmented on the \ac{t2w}-\ac{mri} and thus the latter modality is used as the reference modality.

\subsection{Feature detection}\label{subsec:chp6:method:fea-det}

\begin{table}
  \caption{Features extracted in \acs*{t2w}-\acs*{mri} and \acs*{adc} volumes.}
  \centering
  \scriptsize
  \begin{tabularx}{\textwidth}{lXc}
    \toprule
    \textbf{Features} & Parameters & \# dimensions \\
    \midrule
    Intensity &  & 1 \\
    \acs*{dct} decomposition & window: \SI[product-units=repeat]{9x9x3}{\px} & 243 \\
    Kirsch filter &  & 2 \\
    Laplacian filter &  & 1 \\
    Prewitt filter &  & 3 \\
    Scharr filter &  & 3 \\
    Sobel filter &  & 3 \\
    Gabor filters & 4 frequencies $f \in [0.05, 0.25]$; 4 azimuth angles $\alpha \in [0, \pi]$; 8 elevation angles $\alpha \in [0, 2\pi]$ & 256 \\
    Phase congruency filter & 5 orientations; 6 scales & 3 \\
    Haralick filter & window: \SI[product-units=repeat]{9x9x3}{\px}; \# grey levels: 8; distance: \SI{1}{\px}; 13 directions & 169 \\
    \acs*{lbp} filter & 2 radii $r=\{1, 2\}$; 2 neighborhood sizes $N = \{8, 16\}$ & 3 \\
    \bottomrule
  \end{tabularx}
  \label{tab:featureadct2w}
\end{table}

\begin{figure}
  \hspace*{\fill}
  \subfigure[]{\label{fig:goodfit}\includegraphics[width=.49\textwidth]{6_pipeline/figures/meta_fitting.pdf}}
  \hfill
  \subfigure[]{\label{fig:wrongfit}\includegraphics[width=.49\textwidth]{6_pipeline/figures/meta_fitting_wrong.pdf}}
  \hspace*{\fill}
  \caption[Illustration of the metabolite fitting.]{Illustration of the metabolite fitting: \subref{fig:goodfit} the models are perfectly fitted for both citrate and choline; \subref{fig:wrongfit} the fitting of the citrate metabolite is inaccurate since it does not follow the \textit{a priori} model.}
  \label{fig:fitmeta}
\end{figure}

To approach the task of automatic detection of \ac{cap} using machine learning, one has to extract a variety of feature specific to the \ac{mri} modality as presented in \acs*{sec}\,\ref{subsec:chp3:img-clas:CADX-fea-dec}.

\paragraph{\ac{t2w}-\ac{mri} and \ac{adc} map features}
Apart of using the normalized intensity, edge- and texture-based features are commonly extracted from \ac{t2w}-\ac{mri} and \ac{adc} map.
A set of common features earlier reported in \acs*{sec}\,\ref{subsec:chp3:img-clas:CADX-fea-dec} have been computed.
The following set of filters characterizing edges have been used: (i) Kirsch, (ii) Laplacian, (iii) Prewitt, (iv) Scharr, (v) Sobel, and (vi) Gabor.
Apart of Kirsch filter, the other filters are applied in 3D to get more information using a volume and not a slice, as it is usually done.
The extension of the most common edge detectors in 3D is obvious and will not be recall.
However, 3D Gabor filters~\cite{wang2005face} are not commonly used and we recall their formulation in \acs*{eq}\,\eqref{eq:gabor3d}.

\begin{equation}
  g(\mathbf{x};\boldsymbol{\sigma},f,\theta,\phi) = \hat{g}(\mathbf{x};\boldsymbol{\sigma}) \exp(j 2 \pi f \left( x \sin \theta \cos \phi + y \sin \theta \sin \phi + z \cos \theta \right)) \ ,
  \label{eq:gabor3d}
\end{equation}

\noindent where,

\begin{equation}
  \hat{g}(\mathbf{x};\boldsymbol{\sigma}) = \frac{1}{{\left(2 \pi\right)}^{\frac{3}{2}}} \exp \left( -\frac{1}{2} \left( \frac{x^2}{\sigma_x^2} + \frac{y^2}{\sigma_y^2} + \frac{z^2}{\sigma_z^2} \right) \right) \ ,
  \label{eq:gabor3dgaussian}
\end{equation}

\noindent where $\mathbf{x}$ is the position vector $\{x,y,z\}$, $\boldsymbol{\sigma}$ is the standard deviation vector $\{\sigma_x,\sigma_y,\sigma_z\}$ of the 3D Gaussian envelope, $f$ is the radial center frequency of the sine wave, $\theta$ is the elevation angle, and $\phi$ is the azimuth angle.

Additionally, features based on phase congruency as proposed by \citeauthor{kovesi1999image} are computed~\cite{kovesi1999image}.
Therefore, from a set of Log-Gabor filter bank, the orientation image, the local weighted mean phase angle, and the phase angle are estimated at each voxel.

To characterize the local texture, both second-order \ac{glcm}-based features~\cite{Haralick1973} and rotation invariant and uniform \ac{lbp}~\cite{ojala2002multiresolution}.
To encode 3D information, the 13 first Haralick features --- refer to \acs{tab}~\ref{tab:glcm} --- are computed for the 13 possible directions.
For the same reason, the \ac{lbp} codes are computed for the three-orthogonal-planes of each \ac{mri} volume.

\Acl{tab}~\ref{tab:featureadct2w} summarizes the different features extracted with their corresponding parameters.
Note that all these features are extracted at each voxel of the volume.

\paragraph{\ac{dce}-\ac{mri} features} The extracted features for the \ac{dce}-\ac{mri} are exactly the same than in the previous chapter.
The reader can refer to \ac{sec}\,\ref{subsubsec:chp5:DCE-norm:stateart} for a detailed presentation of the different methods used.
In brief, the entire enhanced signal, semi-quantitative, and quantitative methods are computed. 

\paragraph{\ac{mrsi} features} \ac{mrsi}-based features have been previously explained in \acs{sec}\,\ref{subsubsec:chp3:img-clas:CADX-fea-dec:MRSI-fea}.
Due to unavailability of some unsuppressed water acquisition, absolute quantification as presented by \citeauthor{trigui2017automatic} could not be computed~\cite{trigui2017automatic}.
Therefore, likewise in~\cite{Parfait2012}, two different techniques are used to extract discriminative features: (i) relative quantification based on metabolite quantification, (ii) relative quantification based on bounds integration, and (iii) spectra extraction.

Relative quantification based on metabolite quantification relies on a robust integration of the citrate and choline signal based on peak modelling.
Therefore, we propose to tackle this problem as a non-linear least squares optimization problem by (i) quantifying the citrate peaks as a Gaussian mixture and (ii) quantifying the choline as a single Gaussian.

As illustrated in \acs{fig}\,\ref{fig:goodfit}, the \ac{mrsi} sequence imply a 3-peaks citrate metabolite.
Therefore, we propose the following cost function to represent our function:

\begin{equation}
  C(x) = \alpha_1 \mathcal{N}(x; \mu, \sigma_1) + \alpha_2 \mathcal{N}(x; \mu + \delta_2, \sigma_2) + \alpha_3 \mathcal{N(x; \mu - \delta_3, \sigma_3)} \ ,
\end{equation}

Theoretically, one could suggest to fit a Voigt mixtures instead of a Gaussian mixtures due to the presence of noise during the acquisition.
However, the use of Gaussian distributions reduces the number of parameters to be optimized and allow for a more robust optimization process regarding the choices of the bounds.

\subsection{Feature balancing}\label{subsec:chp6:method:fea-bal}
Data imbalanced is a common problem while classifiying medical data.
Considering a binary classification problem, there is always a class (often the class indicating cancer, or disease patients) that have smallest number of samples (i.e, minority class) in comparison to the other class (i.e, majority class).
The problem of data balancing corresponds to equalize the number of samples of both the minority and majority classes.
This task can be achieved in either data or feature space while performing over-sampling of minority samples or under-sampling of majority samples.
In this section we compare different under and over-sampling techniques applied in feature space.

\subsubsection{\Acl*{us1}}
Techniques that reduce the number of samples of the majority class to be equal to the number of samples of minority class are referred as \ac{us1} techniques.
%Considering the problem of imbalanced, \ac{us} is performed such that the number of samples of the majority class is reduced to be equal to the number of samples of the minority class.
The following methods are considered to perform such balancing.

\begin{description}
  \item[\Ac{rus}] is performed by randomly selecting without replacement a subset of samples from the majority class such that the number of samples is then equal in both minority and majority classes.
  \item[\Ac{nm}] offers three different methods to under-sample the majority class~\cite{mani2003knn}.
In \ac{nm1}, samples from the majority class are selected such that for each sample, the average distance to the $k$ \ac{nn} samples from the minority class is minimum.
\ac{nm2} diverges from \ac{nm1} by considering the $k$ farthest neighbours samples from the minority class.
In \ac{nm3}, a subset $M$ containing samples from the majority class is generated by finding the $m$ \ac{nn} from each sample of the minority class.
Then, samples from the subset $M$ are selected such that for each sample, the average distance to the $k$ \ac{nn} samples from the minority class is maximum.
In our experiment, $k$ and $m$ are fixed to 3.
\item[\Ac{iht}]~\cite{smith2014instance} undersamples the samples which have a high hardness threshold.
Hardness indicates the likelihood of missclassification rate for each samples.
The notation of instance hardness are drawn through the decomposition of $p(h \vert t)$ using Bayes' theorem, where $h$ represent the mapping function used to mapp input features to their corresponding labels and $t$ represents the training set.
\begin{equation}
  IH_h(\langle x_{i}, y_{i}\rangle) = 1 - p(y_i \vert x_i, h).\
\label{eq:iht}
\end{equation}
The undersampling is performed by keeping the most probable samples (i.e, filtering the samples with high hardness value) through \ac{kcv} training sets while considering specific threshold for filtering.
%% The hardness of an instance is depedent on the instances in the training data and the algorithm used to produced h (h is a function mapping input features to their corresponding label , i.e, the classifier, or base learner function).

 
\end{description}

\subsubsection{\Acl*{os}}
Contratry to \ac{us1} techniques, the data balancing can be performed by \ac{os} in which the new samples belonging to the minority class are generated aiming at equalizing the number of samples in both classes.
The following methods are compared for \ac{os} techniques.
\begin{description}
\item[\Ac{ros}] is performed by randomly replicating the samples of the minority class such that the number of samples is equal in both minority and majority classes.
\item[\Ac{smote}] is a method to generate synthetic samples in the feature space~\cite{chawla2002smote}.
Let define $x_i$ as a sample belonging to the minority class.
Let define $x_{nn}$ as a randomly selected sample from the $k$ \ac{nn} of $x_i$, with $k$ set to 3.
Therefore, a new sample $x_j$ is generated such that $x_j = x_i + \sigma \left( x_{nn} - x_i \right)$, where $\sigma$ is a random number in the interval $\left[0,1\right]$.
\item[\Ac{smoteb1}]~\cite{han2005borderline} over samples the minority samples similar to \ac{smote}.
However, instead of usig all the minority samples, it focuses on the borderline samples of minority class.
Borderline samples simply indicate the samples that are closer to the other class.
In this method first the borderline samples of minority class are detected.
The sample, $x_{i}$ belongs to borderline samples if more than half of its $k$ \ac{nn} samples belong to majority class.
Synthetic data is then created based on \ac{smote} method for borderline samples and selection of their $s$ \ac{nn} from the minority class.
 
\item[\Ac{smoteb2}]~\cite{han2005borderline} performs similar to \ac{smoteb1}.
However, instead of generating synthetic samples from borderline samples and their $s$ \ac{nn} from the minority class, it also generate samples based on their \ac{nn} of majority samples.
Synthetic samples created based on majority samples are placed closer to samples of minority class.
\end{description}

\subsection{Feature selection and extraction}\label{subsec:chp6:method:fea-sel}
Different feature selection and extraction methods were presented in Sect.~\ref{subsec:chp3:img-clas:CADX-fea-ext}.
Among those \ac{pca} is used as feature extraction on  \ac{mrsi} and \ac{dce}-based features.
In addition to \ac{pca}, sparse-\ac{pca} and \ac{ica} are also used to extract distinct features from \ac{mrsi} and \ac{dce}-based features.

Similar to \ac{pca}, \ac{ica}~\cite{comon1994independent} is looking for independent components of the datat.
However, it does not require orthogonolaity of the space and does not assume Gaussian distribution for each independent source.
Therefore, opposit to \ac{pca} it can recover uniquely the signals themselve rather than  linear subspace in which the signals lie \cite{murphy2012machine}.

Sparse-\ac{pca}~\cite{zou2006sparse} is another approach for feaure extraction and dimension reduction.
Similar to \ac{pca}, this apprcoach project the data into linear combination of input data.
However, instaed of using original data, it uses the sparse representation of the data, and therefore projects them as linear combination of few input components rather than all of them.
Refereing to Eq.~\eqref{eq:eigpca}, the objective of sparse-\ac{pca} is formulated as maximizing the variance while maintinaing the sparse constraint:

\begin{eqnarray}
 && \argmax \quad   \mathbf{v}^{-1} \Sigma \mathbf{v}\ , \label{eq:sparsepca}\\ 
 && \text{subject to }  \Vert \mathbf{v} \Vert_{2} = 1\ , \nonumber \\
 && \Vert \mathbf{v} \Vert_{0} \leq k\ . \nonumber
\end{eqnarray}
\noindent where $k$ indicates that number of non-zero elements in $\mathbf{v}$.

ANOVA F-test and \ac{rf} are used as feature selection approaches for image-based features.
ANOVA F-test can assess the superiority or inferiority of the features 
ANOVA F-test evaluates the potential of one feature at a time to predict the target label by measuring between group variability over within group variability.
This test finds which features on average are superior of inferior to others versus the null hypothesis.
%% \begin{equation}
%% F = \frac{}{}
%% \label{eq:Ftest}
%% \end{equation} 


\subsection{Classification}\label{subsec:chp6:method:clas}
Variety of classifiers were explained in Sect.~\ref{subsec:chp3:img-clas:CADX-clas}. 
Among those \ac{rf} is used as ensemble approach while classifying inidiviual modalities, where as a meta classifier using stacking of \ac{rf} is used for combination of different modalities.

Staking~\cite{wolpert1992stacked} is a way to create a meta classifier using different base learners.
This method uses the prediction of different base learners as input for a meta leaner and combines them into a final decision.
Each base learner is trained on the training set and its prediction on the validation set is fet to the meta learner.
The test sample, in a similar way is first classified by the base learners and their prediction is passed through the meta learner in order to achieve the final decision.

 
